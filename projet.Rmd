---
title: "Projet TND"
author: "Sutra Suhana (21701858) & Skhiri Nesrine (21709236)"
date: "4/28/2020"
output:
  html_document:
    toc: true
    theme: united
---


## Partie 1 : Analyses descriptives
1.Chargement du jeu de données
```{r cars,message=FALSE}
library(dplyr)
library(leaflet)
library(leaflet.minicharts)
library(htmltools)
library(maps)
library(ggplot2)
library(pastecs)
library(rmarkdown)
donnees = read.csv2(file = "donnes_covid19_avril_total_par_departement_GPS.csv")
donneesAll = donnees %>% mutate(victimeTotale = deces_total + reanimation_total + hospitalises_total + gueris_total) 

paged_table(donneesAll)
```

2. Présentation brève des données

* Description du jeu de données :

  Ce jeu de données comporte 8 variables et 100 individus. Les individus en question sont des départements de la France, on répertorie dans ce jeu de données le nombre de victimes du Covid-19 en fonction des départements sous forme de valeures entières. Il y a alors 4 catégories, les personnes qui sont décédées suite au virus, ceux qui s'en sont sortis guéris du virus, le nombre de personnes qui sont en réanimation et ceux qui sont hospitalisés. Les 4 dernières variables sont le nom du département en question (en String), sa position (latitude et longitude en valeurs réelle) et la duree_jours en valeur entière. Les variables deces_total, reanimation_total, hospitalises_total, gueris_total sont des variables quantitatives continues. La variable maille_nom est qualitative continue, la variable duree_jours est quantitative temporelle, et les variables latitude et longitude sont discrètes.
  
* Analyse descriptive :

  Regardons comment les observations des variables deces_total, reanimation_total, hospitalises_total et gueris_total sont réparties
```{r}
summary(donnees[,5:8])
```
On peut observer grâce au tableau obtenu ci-dessous, que pour chaque variables, il y a un grand écart entre la médiane et la moyenne, cela signifie qu'il y a des individus qui pèsent très "lourd" dans ce jeu de données. De plus, elle est accentuée par le fait que le troisième quartile est "loin" de la valeur maximale. Prenons le cas de la variable deces_totale (c'est le même cas pour les quatres autres variables), cela veut dire que 75% des départements en France ont un nombre de décès inférieur ou égale à 1016. On peut en conclure alors qu'il y a des départements (zones) en France dont les effets sont plus conséquent et que les effets du Covid-19 ne sont pas les mêmes sur le territoire français.

```{r}
boxplot(donnees[,5:8], main="Boîte à moustaches", names=c("décès total","réanimation totale", "hospitali. total","guéris total"), ylab="Nombre d'individus")

boxplot(scale(donnees[,5:8], center = T, scale = T), main="Boîte à moustaches centrée réduite", names=c("décès total","réanimation totale", "hospitali. total","guéris total"), ylab="Nombre d'individus")
boxplot.stats(donnees$deces_total)$out
```
On peut observer la présence de nombreux outliers dans chacune des variables continues(exemple pour la variable deces_total). Nous donc plusieurs valeurs extrêmes et donc peut être abérrantes.
Comme expliqué précedement, on observe également sur le boxplot que pour la variable des hospitalisation_total 75% des départements de France ont un nombre d'hospitalisation inférieur ou égale à 3819. On observe également que 50% des individus de chaque variable est concentrée par rapport au reste de la distribution. Pour la variable hospitalises_total, 50% des départements français ont un nombre d'hospitalisés total entre 840.8 et 3819.5.

3. Cartes géographique des données
```{r, fig.show='hide'}
carteFrance = map('france',col="grey", fill=TRUE, bg="white")
```

```{r, warning=FALSE}
colors =  c("#e31212","#fc7703","#edd21f","#b0e882")

deces_motif <- c(500, 2000, 5000, 10000)

ggplot() + geom_polygon(data = carteFrance, aes(x=long, y = lat, group = group), fill="grey", alpha=1) + ylim(40,52) + coord_map() + 
 xlim(-5,10) + theme_void() + 
 geom_point(data=donnees, aes(x = longitude, y = latitude, size = deces_total,alpha = deces_total), color = colors[1]) + 
 ggtitle("Nombre de décès en France Métropolitaine suite au Covid-19 \n(1/4/20 - 14/4/20)") + 
 scale_size_continuous(name="Nombre de décès", range=c(1,10),breaks = deces_motif) +
 scale_alpha_continuous(name="Nombre de décès", range=c(0.2,1),breaks = deces_motif)

rea_motif <- c(200,1000, 3000, 7000, 10000)

ggplot() + geom_polygon(data = carteFrance, aes(x=long, y = lat, group = group), fill="grey", alpha = 1) + ylim(40,52) + coord_map() +
 xlim(-5,10) + theme_void() + 
 geom_point(data=donnees, aes(x = longitude, y = latitude, size = reanimation_total,alpha = reanimation_total), color = colors[2]) +
 ggtitle("Nombre de personnes en réanimation en France Métropolitaine \nsuite du Covid-19 (1/4/20 - 14/4/20)") + 
 scale_size_continuous(name="Nombre de personne en réanimation", range = c(1,10),breaks = rea_motif) +
 scale_alpha_continuous(name="Nombre de personne en réanimation", range = c(0.1,2),breaks = rea_motif)

 hosp_motif<- c(1000,5000, 10000, 20000, 40000)
 
ggplot() + geom_polygon(data = carteFrance, aes(x=long, y = lat, group = group), fill="grey", alpha = 1) + ylim(40,52) + coord_map() +
 xlim(-5,10) + theme_void() + 
 geom_point(data=donnees, aes(x = longitude, y = latitude, size = hospitalises_total,alpha = hospitalises_total), color = colors[3]) +
 ggtitle("Nombre de personnes hospitalisées en France Métropolitaine \nsuite du Covid-19 (1/4/20 - 14/4/20)") + 
 scale_size_continuous(name="Nombre de personnes hospitalisées", range = c(1,10),breaks = hosp_motif) +
 scale_alpha_continuous(name="Nombre de personnes hospitalisées", range = c(0.2,2),breaks = hosp_motif)

 gueri_motif<- c(500,2000, 5000, 10000, 20000)
 
ggplot() + geom_polygon(data = carteFrance, aes(x=long, y = lat, group = group), fill="grey", alpha = 1) + ylim(40,52) + coord_map() +
 xlim(-5,10) + theme_void() + 
 geom_point(data=donnees, aes(x = longitude, y = latitude, size = gueris_total,alpha = gueris_total), color = colors[4]) +
 ggtitle("Nombre de personnes guéris en France Métropolitaine \nsuite du Covid-19 (1/4/20 - 14/4/20)") + 
 scale_size_continuous(name="Nombre de personnes guéris", range = c(1,10),breaks = gueri_motif) +
 scale_alpha_continuous(name="Nombre de personnes guéris", range = c(0.2,2),breaks = gueri_motif)
```

D'après les différents graphes observés ci-dessus, les zones les plus touchées suite au Covid-19 (personnes décédées, personnes en phase de réanimation, personnes hospitalisées et personnes) sont celles situées en région parisienne ainsi que les départements de l'est. Néanmoins, on peut tout de même observer qu'il y a aussi une forte concentration de personnes touchées dans les départements près de Lyon. Cependant, le nombre de décès à Marseille est inférieure par rapport aux zones citées jusque là mais il compte tout de même un nombre important de personnes en réanimation, de personnes décédées et de personnes guéries. Mise à part cela, on remarque qu'il y a plus ou moins une répartition égale au niveau de la France.


4. Bonus
```{r}
tilesURL <- "http://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Light_Gray_Base/MapServer/tile/{z}/{y}/{x}"
basemap <- leaflet(width = "100%", height = "400px") %>%
  addTiles(tilesURL)

basemap %>% 
  addMinicharts(lng = donneesAll$longitude[], lat = donneesAll$latitude[],type = "pie",
                chartdata = donneesAll[,c("deces_total","reanimation_total","hospitalises_total","gueris_total")],
                width = 50 * sqrt(donneesAll$victimeTotale) / sqrt(max(donneesAll$victimeTotale)),
                colorPalette = colors, legend = FALSE) %>% setView(lng = 2.3522219, lat = 48.856614, zoom = 7) %>% addLegend(
                        colors = colors, opacity = 1,
                        labels = c("Décès", "Réanimations", "Hospitalisées","Guéries")
                        ) %>% addLabelOnlyMarkers(lng = donneesAll$longitude, lat = donneesAll$latitude, label = donneesAll[,1], 
                      labelOptions = labelOptions(noHide = T, direction = 'bottom', textOnly = T,offset = c(2,2)))
```


## Partie 2 : Prédictions du nombre de décès

1. Corrélation des données
``` {r}
library(corrplot)
library(RColorBrewer)

correlation_matrix = cor(donneesAll%>% select(duree_jours,latitude,longitude,deces_total,reanimation_total,hospitalises_total,gueris_total, victimeTotale))
corrplot(correlation_matrix, type='lower',  method = 'number', title="Matrice de corrélation", tl.srt=45, mar=c(0,0,1,0))
corrplot(correlation_matrix, type='lower',  method = 'pie', title="Matrice de corrélation", tl.srt=45, mar=c(0,0,1,0))

```

On observe que les variables deces_total, reanimation_total, hospitalises_total et gueris_total sont fortement corrélés positivement entres elles. Plus précisement, les variables hospitalises_total et reanimation_total sont les variables les plus fortement corréles positivement. Tandis que duree_jours est faiblement corrélés négativement avec toutes les variables. Cependant, on observe que latitude est légèrement corrélée positivement avec les variables deces_total, reanimation_total, hospitalises_total et gueris_total. On peut donc en déduire qu'en fonction de la latitude, le nombre de décès, d'hospitalisés, de réanimation, de guéris et de victime totale augmente comme c'est bien le cas à une latitude Nord (région Parisienne, Grand Est) et une latitude Sud (Lyon, Marseille...). Lorsque la latitude augmente, nous observons donc que plus la région française est au Nord plus le nombre de décès, hospitalisés, guéris et de réanimations augmente également.

2. Nuage de points entre chaque variable et la variable "deces_total"

``` {r}
library(gridExtra)

g1 <- ggplot(donnees, aes(x=duree_jours,y=deces_total)) + geom_point() +ggtitle("Nuage de points jours/décès")+ theme(plot.title = element_text(size=8))

g2 <- ggplot(donnees, aes(x=latitude,y=deces_total)) + geom_point()+ggtitle("Nuage de points latitude/décès")+ theme(plot.title = element_text(size=8))

g3 <- ggplot(donnees, aes(x=longitude,y=deces_total)) + geom_point()+ggtitle("Nuage de points longitude/décès")+ theme(plot.title = element_text(size=8))

g4 <- ggplot(donnees, aes(x=reanimation_total,y=deces_total)) + geom_point()+ggtitle("Nuage de points reanimation/décès")+ theme(plot.title = element_text(size=8))

g5 <- ggplot(donnees, aes(x=hospitalises_total,y=deces_total)) + geom_point()+ggtitle("Nuage de points hospitalisés/décès")+ theme(plot.title = element_text(size=8))

g6 <- ggplot(donnees, aes(x=gueris_total,y=deces_total)) + geom_point()+ggtitle("Nuage de points guéris/décès")+ theme(plot.title = element_text(size=8))

grid.arrange(g1, g2, g3, g4,g5,g6, nrow = 2)

```

On remarque que les variables duree_jours, latitude, et longitude sont assez dispersées. 
Cependant les variables reanimation_total, hospitalise_total et gueris total semblent expliquer au mieux la variable à prédire deces_total. En effet, on observe une certaine corrélation positive entre chaque variables et la variable à prédire. 

3. Division du jeu en deux ensemble apprentissage/test (80-20%)

``` {r}

sample <- sample.int(n=nrow(donnees), size=floor(.80*nrow(donnees)), replace = F)
train <- donnees[sample, ]
test  <- donnees[-sample, ]
```


4. Régression linéaire

##### Valeur explicative : reanimation_total / Valeur prédite : deces_total
``` {r}
reg_rea <- lm(deces_total ~ reanimation_total, data = train)
plot(train$reanimation_total, train$deces_total)
abline(reg_rea, col='red')
plot(reg_rea)

```
      
  * Residuals vs fitted:
  
     La courbe de régression locale est presque plate et les points sont presques tous dispersés de manière égale autour de la droite horizontale (Residuals=0). On peut en déduire que le modèle de régression est acceptée.
      
  * Normal Q-Q plot: 
  
    Les résidus se rapprochent de la droite mais s'éloignent aux extrémités. On peut donc en déduire que le modèle est moyennement bon.
    
  * Scale-Location:
      
    La courbe rouge est ascendante, cela signifie donc que la dispersion des résidus n'est pas homogène. Dans ce cas, le modèle n'est pas bon.
    
  * Residuals vs Levarage:
    
    Ce graphique va nous permettre d'évaluer l'influence des outliers dans les résultats de la régression. On observe deux points 73 et 71 qui sont à la limite de la cook's distance. Cependant, ils ne sont pas au delà des deux limites de la cook's distance. Ces valeurs extrêmes n'ont donc pas une grande influence sur la droite de régression. C'est donc un cas avec des bruits sans influence.
    
    
      
##### Valeur explicative : hospitalises_total / Valeur prédite : deces_total

``` {r}

reg_hospi <- lm(deces_total ~ hospitalises_total, data = train)
plot(train$hospitalises_total, train$deces_total)
abline(reg_hospi, col='red')
plot(reg_hospi)

```

    
  * Residuals vs fitted:
  
     La courbe de régression locale est presque plate et les points sont presques tous dispersés de manière égale autour de la droite horizontale (Residuals=0). Il y a seulement quelques valeurs extrêmes après 20 000. On peut en déduire que le modèle de régression est acceptée. 
      
  * Normal Q-Q plot: 
  
    Les résidus se rapprochent de la droite mais s'éloignent aux extrémités de manière ascendante. On peut donc en déduire que le modèle est moyennement bon.
    
  * Scale-Location:
      
    La courbe rouge est ascendante,les résidus se dispersent après la valeur 0. Cela signifie donc que la dispersion des résidus n'est pas homogène. Dans ce cas, le modèle n'est pas bon.
    
  * Residuals vs Levarage:
    
    On observe deux points 71 et 92 qui sont à la limite extérieur de la cook's distance. Ce sont tout deux des points influents, ils ont donc une grande influence sur la droite de régression. Les coeffecients et la valeur de b  de l'ordonnée à l'origine peuvent changer de manière significative. Il faudrait donc le retirer et refaire un Residuals vs Levarage sans ses outliers. C'est donc un cas avec des bruits avec influence.
    
    


##### Valeur explicative : gueris_total / Valeur prédite : deces_total

``` {r}

reg_gueris <- lm(deces_total ~ gueris_total, data = train)
plot(train$gueris_total, train$deces_total)
abline(reg_gueris, col='red')
plot(reg_gueris)

```

  * Residuals vs fitted:
  
     La courbe de régression locale est presque plate et les points sont presques tous dispersés de manière égale autour de la droite horizontale (Residuals=0). Il y a seulement quelques valeurs extrêmes après 3 000. On peut en déduire que le modèle de régression est acceptée. 
      
  * Normal Q-Q plot: 
  
    Les résidus se rapprochent de la droite mais s'éloignent aux extrémités de manière ascendante. On peut donc en déduire que le modèle n'est pas vraiment bon.
    
  * Scale-Location:
      
    La courbe rouge est ascendante,les résidus se dispersent après la valeur 0. Cela signifie donc que la dispersion des résidus n'est pas homogène. Dans ce cas, le modèle n'est pas bon.
    
  * Residuals vs Levarage:
    
    On observe deux points 87 et 92 qui sont à la limite extérieur de la cook's distance. Cependant, ils ne sont pas au delà des deux limites de la cook's distance. Ces valeurs extrêmes n'ont donc pas une grande influence sur la droite de régression. On observe également un autre point, le point 71 qui est entre les deux Cook's distance supérieur. Mais, il reste tout de même à l'intérieur des lignes pointilles externes, il n'a donc pas une grande influence sur la droite de régression. C'est un cas avec des bruits sans influence.
    
  
5. Commentaires des résultats de la régression

``` {r}
reg_total <- lm(deces_total ~ gueris_total + reanimation_total+hospitalises_total, data = train)
summary(reg_total)
```
    La médiane des résidus est de -20.68 ce qui reste proche de 0 par rapport au maximum et minimum de respectivement 1645.75 et -1932.5 . On observe également que notre premier quartile et notre troisième quartile ont presque la même valeur absolue. Les résidus sont donc distribués selon une loi normale. Lorsque le nombre de décès augmente d'une unité, le nombre de réanimations diminue de 0.43, le nombre d'hospitalisation augmente de 0.29 et le nombre de guérison ne varient que de 0.05 .

    
6. Erreur MAE et MSE


``` {r, warning=FALSE}

MAE <- function(y_reel, y_est){
  val = abs(y_reel-y_est)
  mae_cal = mean(val)
  return(mae_cal)
  
}

MSE <- function(y_reel, y_est){
  val = (y_reel-y_est)^2
  mse_cal = mean(val)
  return(mse_cal)
  
}

#MAE et MSE reanimation_total ensemble apprentissage
y_estime_rea=predict(reg_rea)
MAE_rea= MAE(train$reanimation_total,y_estime_rea)
MSE_rea= MSE(train$reanimation_total,y_estime_rea)


#MAE et MSE hospitalises_total ensemble apprentissage
y_estime_hosp=predict(reg_hospi)
MAE_hosp= MAE(train$hospitalises_total,y_estime_hosp)
MSE_hosp= MSE(train$hospitalises_total,y_estime_hosp)


#MAE et MSE gueris_total ensemble apprentissage
y_estime_gueris=predict(reg_gueris)
MAE_gueris= MAE(train$hospitalises_total,y_estime_gueris)
MSE_gueris= MSE(train$hospitalises_total,y_estime_gueris)


#MAE et MSE reanimation_total ensemble test
y_estime_rea_test=predict(lm(donnees$deces_total ~ donnees$reanimation_total), test)
MAE_rea_test=MAE(test$reanimation_total,y_estime_rea_test)
MSE_rea_test= MSE(test$reanimation_total,y_estime_rea_test)


#MAE et MSE hospitalises_total ensemble test
y_estime_hospi_test=predict(lm(donnees$deces_total ~ donnees$hospitalises_total), test)
MAE_hospi_test=MAE(test$hospitalises_total,y_estime_hospi_test)
MSE_hospi_test= MSE(test$hospitalises_total,y_estime_hospi_test)


#MAE et MSE gueris_total ensemble test
y_estime_gueris_test=predict(lm(donnees$deces_total ~ donnees$gueris_total), test)
MAE_gueris_test=MAE(test$gueris_total,y_estime_gueris_test)
MSE_gueris_test= MSE(test$gueris_total,y_estime_gueris_test)


df <- data_frame(Variables=c("guéris", "hospitalisés", "réanimation"),
MAE_test=c(MAE_gueris_test,MAE_hospi_test,MAE_rea_test),
MSE_test=c(MSE_gueris_test,MSE_hospi_test,MSE_rea_test),
MAE_app=c(MAE_gueris, MAE_hosp, MAE_rea),
MSE_app=c(MSE_gueris,MSE_hosp,MSE_rea))
paged_table(df)
```
7. à faire


## Partie 3 : Clustering des départements selon la dynamique de propagation du virus

```{r}
library(FactoMineR)
library(ggplot2)

```

1.
```{r}
pca = PCA(donnees[,-1],scale.unit = T,graph = F)

PC1 = pca$ind$coord[,1]
PC2 = pca$ind$coord[,2]
labs = rownames(pca$ind$coord)

compos = data.frame(cbind(PC1,PC2))
rownames(compos) = labs

ggplot(compos, aes(PC1,PC2, label=rownames(compos))) + geom_point()

```

Après analyse, on peut observer dans un premier temps dans le nuage des individius que deux groupes (voir trois groupes) se forment. Il y a en effet un groupe très condensé d'individu vers le centre du graphe et un autre groupe moins condensé voir presque aligné à l'axe de la première composante.
On peut aussi observer un autre groupe d'individus moins condensé .

``` {r}
plot(pca,choix = "var")
```

Observons maintenant le cercle des coréllations, le plan d'inertie maximum receuille environ 70% de l'inertie totale.
On peut observer que les variables deces_total, gueris_total, hospitalises_total et reanimation_total sont très corrélées à la première composante, en effet celles-ci sont limites parrallèle à l'axe C1. La variable longitude est également très corrélées à la deuxième composante.

Globalement, on voit que la première composante retranscrit majoritairement les informations liées au victimes du Covid-19 (deces_total,gueris_total, hospitalises_total et reanimation_total) tandis que C2 traduit majoritaire la longitude.

```{r}
pca$var$contrib
```
Pour la contribution des variables, on peut voir que les variable ayant une relation avec le Covid-19 ont effectivement contribué et que cette contribuation est assez égales (aux alentours de 24% - 25%) entre eux. Quant à la deuxième composante, la variable duree_jours à contribué plus de 45%.
```{r}
pca$eig[,3]
```
On peut voir qu'avec seulement les 2 premières composantes, on recueille plus de 70% de l'inertie totale et qu'en associant la troisième composante, plus de 84% de l'inertie totale est alors enregistré.

2.
```{r}
ggplot(compos, aes(PC1, PC2, label = rownames(compos))) + geom_point(aes(color = donnees[,"deces_total"])) + scale_color_gradient(name="Nombre de décès",low = "#00AFBB", high = "#fc0000") + ggtitle("Projection des individus suivant la variable décès")
``` 

En observant la disposition des individus sur le graphe, on peut observer des groupes d'individu regroupé par couleurs rouge, orange et bleu-gris. Les département ayant le nombre de décès faible représente le groupe très condensé (en bleu) tandis que le groupe moins condensé en orange représente les département ayant un nombre totale de décès aux alentours de 5000. D'après ce graphe, seule un département à un nombre de total décès supérieur à 10000. Suivant la variable décès et le graphique obtenu, plus le nombre de décès  dans le département est conséquent, plus il est rouge et disposé à gauvhe Moins le nombre de décès est conséquent, la couleur du point devient alors bleu et est disposé à droite. Laes départements sont alors disposés de droite à gauche selon leur nombre de décès du Covid-19

3.

Réalisons un classification hiérarchique ascendante afin de répartir les individus dans un certain nombre de classe, nous allons pour cela utiliser les 4 critères à savoir lien minimum, moyen, maximum et Ward, nous allons par la suite regarder les sauts d'inertie de chaque dendograme 
```{r}
library(ggplot2)
datadist = dist(x = donnees[,-1],method = "euclidean")
```


```{r}
#En utilisant le critère lien minimun
dataclustm = hclust(d = datadist,method = "single")
sautInertie = sort(dataclustm$height, decreasing = TRUE)
plot(dataclustm,main = "Dendograme construit grace à la méthode d'agrégation lien minimum")
plot(sautInertie[1:15],type = "s", xlab = "Nombre de classes", ylab = "Inertie",main = "Représentation des sauts d'inertie du dendogramme lien minimum")
plot(dataclustm, hang = -1, cex=0.4)
cahm <- cutree(dataclustm,k=3)
rect.hclust(dataclustm, k=3, border="red")

```


```{r}
#En utilisant le critère lien maximun
dataclustM = hclust(d = datadist,method = "complete")
sautInertie = sort(dataclustM$height, decreasing = TRUE)
plot(dataclustM,main = "Dendogramme construit grace à la méthode d'agrégation lien maximum")
plot(sautInertie[1:15],type = "s", xlab = "Nombre de classes", ylab = "Inertie",main = "Représentation des sauts d'inertie du dendogramme lien maximum")
plot(dataclustM, hang = -1, cex=0.4,main = "Dendogramme construit grace à la méthode d'agrégation lien maximum")
cahM <- cutree(dataclustM,k=3)
rect.hclust(dataclustM, k=3, border="red")

```


```{r}
#En utilisant le critère lien moyen
dataclustAV = hclust(d = datadist,method = "average")
sautInertie = sort(dataclustAV$height, decreasing = TRUE)
plot(dataclustAV,main = "Dendogramme construit grace à la méthode d'agrégation lien moyen")
plot(sautInertie[1:15],type = "s", xlab = "Nombre de classes", ylab = "Inertie",main = "Représentation des sauts d'inertie du dendogramme lien moyen")
plot(dataclustAV, hang = -1, cex=0.4,main = "Dendogramme construit grace à la méthode d'agrégation lien moyen")
cahAV <- cutree(dataclustAV,k=3)
rect.hclust(dataclustAV, k=3, border="red")
```


```{r}
#En utilisant le critère de Ward
dataclustW = hclust(d = datadist,method = "ward.D2")
sautInertie = sort(dataclustW$height, decreasing = TRUE)

plot(dataclustW,main = "Dendograme construit grace à la méthode d'agrégation Ward")
rect.hclust(dataclustW,3,border = "blue")
plot(sautInertie[1:15],type = "s", xlab = "Nombre de classes", ylab = "Inertie", main = "Représentation des sauts d'inertie du dendogramme Ward")

cahW <- cutree(dataclustW,k=3)

```
Pour pouvoir trouver le nombre de classe à partir après réalisation d'une CAH, on affiche l'arbre hierachique obtenue, puis on regarde en partant du bas, à partir de quelle hauteur, les distances d'aggregations commencent à croître drastiquement. Pour nous aider, on peut représenter les sauts d'inerties du dendogrammes selon les classes retenues. On regarde ainsi les coudes que le graphe possède et puis on retient la hauteur de ce coude et on le coupe à l'arbre.

Parmis les différents critères, le dendogramme obtenu avec le critère de Ward nous donne des groupes qui sont assez homogènes par rapport aux autres critères et qu'on peut bien différencier les groupes à une certaine hauteur. 3 groupes ont été alors identifié dans le dendogramme selon le critère de Ward.

Réalisons maintenant un clustering en k-means qui nous permettra également de répartir les individus en classes bien distinctes.
```{r, warning=FALSE}
library(factoextra)
library(cluster)
library(gridExtra)

#K-means

k2 <- kmeans(donnees[,-1], centers = 2, nstart = 30)
k3 <- kmeans(donnees[,-1], centers = 3, nstart = 30)
k4 <- kmeans(donnees[,-1], centers = 4, nstart = 30)
k5 <- kmeans(donnees[,-1], centers = 5, nstart = 30)

plot(pca$ind$coord, col=k3$cluster, main="ACp/Kmeans")

fviz_nbclust(donnees[,-1], kmeans, method = "wss")

p1 <- fviz_cluster(k2, geom = "point", data = donnees[,-1]) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = donnees[,-1]) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = donnees[,-1]) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = donnees[,-1]) + ggtitle("k = 5")


grid.arrange(p1, p2, p3, p4, nrow = 2)
```
C
ontrairement à l'algorithme CAH, l'algorithme K-means nous oblige à choisir le nombre de clusters. Nous avons donc appliquer kmeans à nos données avec un nombre de classes variant de 2 à 5. Nous pouvons ainsi observer différents graph représentant chaque clusters. 
* On remarque dans le graph k=2 que le groupe à gauche contient des individus très hétérogènes d'où sa forme élargie. 
* Dans le graph k=3, on observe bien 3 clusters homogènes et différents les uns des autres. 
* Dans le graph k=4, on observe que les deux clusters à droite sont très proches et se confondent presque. Mais les clusters restent assez homogènes.
* Dans le graph k=5, on observe une nouvelle fois deux clusters à droite très proches. On remarque également que certaines données sont à la fois dans le cluster 2 et le cluster 5. 

On peut donc en déduire que le nombre de classes idéales est 3. On vérifie nos interprétations avec la méthode du coude. L'emplacement du coude dans le graphique est généralement un indicateur du nombre approprié de clusters. On observe bien un coude entre 3 et 4 clusters.


4. Table de confusion
```{r }
table(k3$cluster,cahm)
table(k3$cluster,cahM)
table(k3$cluster,cahAV)
table(k3$cluster,cahW)
```
Les résultats sont tout de même assez proches, il y a des valeurs qui diffère pour peu mais dans la globalité, on a bien trois groupe avec un presque même repartition que ça soit avec la méthode Kmeans ou CAH.

```{r }
group = cutree(dataclustW, k = 3)
p = ggplot(compos, aes(PC1, PC2)) + geom_point(aes(color=as.factor(group))) + ggtitle("Projection des individus suivant les groupes obtenus par la méthode CAH")
update_labels(p, list(colour="numéro groupe"))

groupeK = k3$cluster
ggplot(compos, aes(PC1, PC2)) + geom_point(aes(color=as.factor(groupeK))) + ggtitle("Projection des individus suivant les groupes obtenus par la méthode Kmeans")
```

Les graphiques dessus nous montre bien que les groupes sont répartie de la même façon que ça soit avec l'algorithme des K-Means ou bien en faisant une classification hiearchique. Les groupes qui sont présent sur les graphiques du dessus correspond plus ou moins au groupe identifié à la première question.

